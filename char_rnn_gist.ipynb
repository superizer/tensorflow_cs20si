{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clean, no_frills character-level generative language model.\n",
    "Created by Danijar Hafner (danijar.com), edited by Chip Huyen\n",
    "for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "Based on Andrej Karpathy's blog: \n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/arvix_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "TEMPRATURE = 0.7\n",
    "LR = 0.003\n",
    "LEN_GENERATED = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS//2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, overlap):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    in_state = tf.placeholder_with_default(\n",
    "            cell.zero_state(tf.shape(seq)[0], tf.float32), [None, hidden_size])\n",
    "    # this line to calculate the real length of seq\n",
    "    # all seq are padded to be of the same length which is NUM_STEPS\n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    return output, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    # fully_connected is syntactic sugar for tf.matmul(w, output) + b\n",
    "    # it will create w and b for us\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1], labels=seq[:, 1:]))\n",
    "    # sample the next character from Maxwell-Boltzmann Distribution with temperature temp\n",
    "    # it works equally well without tf.exp\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0] \n",
    "    return loss, sample, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/arvix/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        iteration = global_step.eval()\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_inference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                saver.save(sess, 'checkpoints/arvix/char-rnn', iteration)\n",
    "            iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def online_inference(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "    \"\"\"\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        # for the first decoder step, the state is None\n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39. \n",
      "    Loss 9412.35546875. Time 8.740547895431519\n",
      "T_FX   e  e  e                                                                                              e     e  e  e        e  e  e    e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e  e \n",
      "Iter 79. \n",
      "    Loss 8205.4765625. Time 8.216433763504028\n",
      "TX te the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Iter 119. \n",
      "    Loss 7403.85205078125. Time 8.627424001693726\n",
      "The the the the the the the the the the the the the the the the the seraling and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and \n",
      "Iter 159. \n",
      "    Loss 6964.90478515625. Time 7.940148115158081\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 199. \n",
      "    Loss 6370.42724609375. Time 9.023845911026001\n",
      "The convexting and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and ather and at\n",
      "Iter 239. \n",
      "    Loss 6111.6884765625. Time 7.838426828384399\n",
      "The and and and and and and and and and and and and and and and and and and approach and and and and and and and and and and and and approach and and and and and and and and and approach and and and and and and and and approach and and and and and and and and and and and and approach and and and and \n",
      "Iter 279. \n",
      "    Loss 5870.76953125. Time 8.166371822357178\n",
      "The aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral aral ar\n",
      "Iter 319. \n",
      "    Loss 6057.20703125. Time 8.652777910232544\n",
      "The aralizen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen and the experinen \n",
      "Iter 359. \n",
      "    Loss 5364.513671875. Time 8.367346048355103\n",
      "The stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the stacks the \n",
      "Iter 399. \n",
      "    Loss 5196.349609375. Time 7.861160755157471\n",
      "The aralenes and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and and \n",
      "Iter 439. \n",
      "    Loss 4988.4150390625. Time 9.096784114837646\n",
      "The and the results of the and the results of the and the results of the and the results of the and the results of the and the results of the and the results of the and the results of the and the results of the and the results of the and the results of the and the results of the and the results of th\n",
      "Iter 479. \n",
      "    Loss 4406.2021484375. Time 8.392661094665527\n",
      "The stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking the stacking th\n",
      "Iter 519. \n",
      "    Loss 4536.68798828125. Time 8.1129789352417\n",
      "The and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the network and the n\n",
      "Iter 559. \n",
      "    Loss 5172.595703125. Time 9.629666090011597\n",
      "The and experiments of the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the in the i\n",
      "Iter 599. \n",
      "    Loss 4418.90625. Time 9.345448970794678\n",
      "The architecture that a simple architecture that the such as the proposed by the network (DNN) have been a deep neural networks (DNNs) and show that the supervised learning the such as the proposed by the network (DNN) have been a deep neural networks (DNNs) and show that the supervised learning the \n",
      "Iter 639. \n",
      "    Loss 3912.7529296875. Time 8.867313861846924\n",
      "The ara layer of stackes the resser convergence rates of the ara layer of stackes the resser convergence rates of the ara layer of stackes the resser convergence rates of the ara layer of stackes the resser convergence rates of the ara layer of stackes the resser convergence rates of the ara layer of\n",
      "Iter 679. \n",
      "    Loss 4106.5458984375. Time 8.450693845748901\n",
      "The proposed model and the proposed model and the state-of-the-art results on the and the state-of-the-art results on the and the state-of-the-art results on the and the state-of-the-art results on the and the state-of-the-art results on the and the state-of-the-art results on the and the state-of-th\n",
      "Iter 719. \n",
      "    Loss 3569.71630859375. Time 7.914686918258667\n",
      "The proposed by the network and parameters to model the mansion for deep neural networks (DNNs) and stachart conventional content of the network propestive to intred exploin the network propestive the network propestive the network propestive the network propestive the network propestive the network \n",
      "Iter 759. \n",
      "    Loss 3981.5986328125. Time 9.908336877822876\n",
      "The contiguted to the context of the straightiti) in the context of the straightiti) in the context of the straightiti) in the context of the straightiti) in the context of the straightiti) in the context of the straightiti) in the context of the straightiti) in the context of the straightiti) in the\n",
      "Iter 799. \n",
      "    Loss 3979.11376953125. Time 9.14783501625061\n",
      "The speech recognition system is the state-of-the-art ferword from the speech recognition system is the state-of-the-art ferword from the speech recognition system is the state-of-the-art ferword from the speech recognition system is the state-of-the-art ferword from the speech recognition system is \n",
      "Iter 839. \n",
      "    Loss 3479.8671875. Time 7.992035865783691\n",
      "The context of deep learning architectures that are in the training deep neural networks and the network problems of the and theoretical structure of the and theoretical structure of the and theoretical structure of the and theoretical structure of the and theoretical structure of the and theoretical\n",
      "Iter 879. \n",
      "    Loss 3394.515625. Time 7.869065999984741\n",
      "The context of deep learning results of a simple models in a single model to a contrast the compount of the experiments of a simple models in a single model to a contrast the the ensimiled distribution of the experiments of a simple models in a single model to a contrast the the ensimiled distributio\n",
      "Iter 919. \n",
      "    Loss 3357.814453125. Time 8.193148851394653\n",
      "The from the expending the successfully computation is the supervised learning relative models and show that the experse in the successfully cornections that the successfully computation in the the deep neural networks (DNNs) and show that the experse in the successfully cornections that the successf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 959. \n",
      "    Loss 3589.32275390625. Time 9.002385139465332\n",
      "The computational results on a simple convergence rates of the network problem in over general waik deep neural networks to learn a deep networks that are computation in the convergence rates of the network problem in the convergence rates of the network problem in the convergence rates of the networ\n",
      "Iter 999. \n",
      "    Loss 3225.7978515625. Time 8.245115041732788\n",
      "The speech recognition to train a deep learning machine learning and the state-of-the-art on the speed up the state-of-the-art on the speed up the state-of-the-art on the speed up the state-of-the-art on the speed up the state-of-the-art on the speed up the state-of-the-art on the speed up the state-\n",
      "Iter 1039. \n",
      "    Loss 3577.48828125. Time 9.034195184707642\n",
      "The massively a propose a new architectures shaling to a particlly learned for the to the sequence of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of the network architecture of the\n",
      "Iter 1079. \n",
      "    Loss 3199.54931640625. Time 8.672798156738281\n",
      "These in a single machine learning tasks of learning algorithms and such as a neural network that is importart in a simple nour networks (DNN) however, and the encoder in a deep neural network that is importart dorain to be a difficult experiments of a simple nour gradient descent (SVRR) methods for \n",
      "Iter 1119. \n",
      "    Loss 3451.46826171875. Time 7.92825174331665\n",
      "The actions the actions the speech recognition systems with a similar to a simple matrices to a similated by the and computational existing deep learning in the and for a corsect of group the interentional state-of-the-art results in the and from training datasets. We show that the approach to train \n",
      "Iter 1159. \n",
      "    Loss 2947.65283203125. Time 8.805092811584473\n",
      "The computation of the algorithms the computation of the algorithms the computation of the algorithms the computation of the input of the arcaitien to learn a deep network architectures and the ontained for training standard approaches for training and computation of the input of the arcaitien for tr\n",
      "Iter 1199. \n",
      "    Loss 3694.697265625. Time 9.031797885894775\n",
      "The state-of-the-art descent and state-of-the-art descent an experiments lemess of deep learning algorithms to train a single layer with standard backpropagation of the structure of the structure of the structure of the structure of the structure of the structure of the structure of the structure of \n",
      "Iter 1239. \n",
      "    Loss 3269.80615234375. Time 7.94511604309082\n",
      "The set of computations of the supervised learning recent recurrent neural networks (DNNs) are not ofly interparale to a contral optimization proposed method for large shased model as a simple mathine of the supervised learning recent recurrent neural networks (DNNs) are not ofly interparale to a con\n",
      "Iter 1279. \n",
      "    Loss 3284.34130859375. Time 8.242083072662354\n",
      "The convolutional neural networks that the convolutional networks that are complexes of straightick datasets the predictions and the network deppheancer neural networks that the convolutional networks that are complicated by computation of the network deppheancer a special speech recognition and the \n",
      "Iter 1319. \n",
      "    Loss 3318.2919921875. Time 8.531653881072998\n",
      "The approach for the exploding neural networks (DNN) that our ant of convergence of the and conventional concepts on a stablic groduces and maxtutes for stale different distribution efficiency of the data deperdingting in the data etwodks (CIF) that our and the effectiveness and the different distrib\n",
      "Iter 1359. \n",
      "    Loss 2822.49365234375. Time 8.378947973251343\n",
      "The functions, provide a generalizes that the functions, and show that the proposed method for training deep neural networks (DNNs) as achieve the model size of the success of the success of the success of the success of the success of the success of the success of the success of the success of the s\n",
      "Iter 1399. \n",
      "    Loss 3183.737060546875. Time 9.162970066070557\n",
      "The computation of the network to learn have been such as han been in a general parameters and restricted experiment demany, a deep neural networks is a hied-noul-sigiting architectures with nonconvex optimization. We the results showe gradient descent that can be erficientity and experimentally to e\n",
      "Iter 1439. \n",
      "    Loss 3412.17138671875. Time 8.698760032653809\n",
      "The effectiveness and and the state-of-the-art results such as the state-of-the-art results such as the state-of-the-art results such as the state-of-the-art results such as the state-of-the-art results such as the state-of-the-art results such as the state-of-the-art results such as the state-of-the\n",
      "Iter 1479. \n",
      "    Loss 3066.092529296875. Time 7.475409746170044\n",
      "The recognition of the activation for training deep neural networks as a problems of the recognition of the activation for training deep neural networks and successful the recently pooling operations for theory a standariay of the activation for training deep neural networks and successful the recent\n",
      "Iter 1519. \n",
      "    Loss 3292.86279296875. Time 8.634877920150757\n",
      "The convergence rates of the standard and stalde the state-of-the-art networks are a time setionis models in the convergence rates of the structures stald not unit for example, in the convergence rates of the standard and inate and standard and the structure of the composing the convergence rates of \n",
      "Iter 1559. \n",
      "    Loss 2891.77880859375. Time 7.842595815658569\n",
      "The set of experiments such as intermaniing interest training of the same to-the set of experiments such as inference to show the empirical experiments designed by the sequence of the art state-of-the-art on changet of the input of the activation function to the proposed RNN training the activation f\n",
      "Iter 1599. \n",
      "    Loss 3076.3251953125. Time 7.3046791553497314\n",
      "The recurrent neural network (RNN) architecture trains the masking functions that recent lowe convergence to address the proposed method using a soncenss with a convolutional neural networks and and training set of computation of the matrix can be applied to a deep convolutional neural networks and a\n",
      "Iter 1639. \n",
      "    Loss 2710.489501953125. Time 7.036435842514038\n",
      "The full model and recognition state-of-the-art portwork structured by the deep learning algorithm for theory on the input shaper iner miximizarion problem in the data for training deep neural networks (RBNs have y ever and factorization of the difficult experiments demonstrating in the function and \n",
      "Iter 1679. \n",
      "    Loss 2962.86865234375. Time 7.526730060577393\n",
      "The recently proposed method used in the search for argoes to a contral rule of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success\n",
      "Iter 1719. \n",
      "    Loss 2715.4482421875. Time 6.934391975402832\n",
      "The approach iath error rates to achieve a convexity interpreting a deep networks in the network depth and image classification training of deep neural networks with a method to our experiment lear imen in a distributed on resterized Bay not only a similar a context of deep neural networks with a met\n",
      "Iter 1759. \n",
      "    Loss 3101.786376953125. Time 6.991760969161987\n",
      "The intermedia a different to a deep learning algorithm is to be the decoder and parameter spaces of stochastic an a matic models of a large using a mations of the intermedias the pooling operation of the input of the approach to be the decoder and parameter spaces of stochastic an a matic models of \n",
      "Iter 1799. \n",
      "    Loss 2804.9189453125. Time 7.3201422691345215\n",
      "The recurrent neural networks and a tranding the content of the context of performance of the training of the training of the training of the training of the training of the training of the training of the training of the training of the training of the training of the training of the training of the\n",
      "Iter 1839. \n",
      "    Loss 2758.02734375. Time 7.478921175003052\n",
      "The computation into the convex optimization processing. We provides the convex optimization of the network size of the network size of the network size of the network size of the network size of the network size of the network size of the network size of the network size of the network size of the n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1879. \n",
      "    Loss 2834.0107421875. Time 8.23366379737854\n",
      "The spare of the art on the explore the approach capled the training and the different trained with the about 10 can be used to train and the classification performance on a single machine learning tasks size of the and theory is decoding learning to a single machine learning tasks. However, improves\n",
      "Iter 1919. \n",
      "    Loss 2757.153076171875. Time 7.796314001083374\n",
      "The recurrent neural networks are alts in the recent results on a method for training deep neural networks (RNNs) as a probabilistic framework for each neuron learning architectures by a pre-train non-convex optimization are standard formation of the arcaiest of pointic models and formal sentiment an\n",
      "Iter 1959. \n",
      "    Loss 2880.18505859375. Time 7.692707777023315\n",
      "The infinet from the layers and a new approach for posterior problem in the for derension for state-of-the-art model of the state-of-the-art model for the layers and a new approach for posterior problem in the for derension for state-of-the-art model of the state-of-the-art model for the layers and a\n",
      "Iter 1999. \n",
      "    Loss 2721.722412109375. Time 8.420395135879517\n",
      "The search for sequence of the same offer networks of the speech recognition, we propose and computations of the input samples gram hincers of architecture of the speech recognition. We introduce a new architectures and a new architectures and a new architectures and a new accessive model architectur\n",
      "Iter 2039. \n",
      "    Loss 2666.35498046875. Time 7.547176837921143\n",
      "The compositional firteraly learn to dependent network connections that are computational layers to be achieve has the proposed method convergence to an exploims. The proposed model can be achieve the embed points and then information problems that are computational layers, and the network problems t\n",
      "Iter 2079. \n",
      "    Loss 2916.093017578125. Time 8.181069135665894\n",
      "The first on speed up ally the desire to a defined multi-layers and factors (RP) for each features from the introduced to train minimal on the considerity and state-of-the-art of the distribution for data from the parameters (semp technique to learn different minimization to the designed to present i\n",
      "Iter 2119. \n",
      "    Loss 2669.86181640625. Time 9.033849716186523\n",
      "The set of computational results such as representations of the sequence of the set of computational results such as unit rectifier learning algorithms that dereviss of sequented by a pre-training deep neural networks (DNNs) as achieve the generalization of the sequence of the set of computational re\n",
      "Iter 2159. \n",
      "    Loss 2725.80517578125. Time 7.731446981430054\n",
      "The approach in the network, a convolutional neural networks with nonconvex optimization processing layers, such as handwriting rester impostable convergence rates of the network's Jaigiting are standard approach to improve the networks on the network's Jaighiar recognition systems with chind term de\n",
      "Iter 2199. \n",
      "    Loss 2788.7802734375. Time 7.725595951080322\n",
      "The approach further information of the input data space that the training and maxout units can be trained to train a senorm are allowing the input datangs the existing that the function accuracy of SVRG for convex that maxomem to optimize the about the effectiveness on a stape different to a differe\n",
      "Iter 2239. \n",
      "    Loss 2806.19189453125. Time 8.511929988861084\n",
      "The recognition systems structure of rectifiers compatition to the context of deep neural networks. These proposed models in a deep neural networks and a recurrent neural networks (DNNs) as a probabilistic framework for each paphlywork approach such as the recurrent neural networks. We provide a pres\n",
      "Iter 2279. \n",
      "    Loss 2619.236328125. Time 7.509387016296387\n",
      "The approach for derived to state of the structure of recent recognition to the detice recognition of the state-of-the-art results on the convergence rate of the state-of-the-art results on the convergence rate of the state-of-the-art results on the convergence rate of the state-of-the-art results on\n",
      "Iter 2319. \n",
      "    Loss 2596.799072265625. Time 7.2086169719696045\n",
      "The approach for examile data of the training of a single machine learning to a single model and the training and maxout units ker each layer of the about unsienal can oble to a different descent to a single model and the training and maxout units ker each layer of the about unsienal can oble to a di\n",
      "Iter 2359. \n",
      "    Loss 2516.602294921875. Time 7.003232002258301\n",
      "The approach io semplo encoders and analyzing a new prediction and computation of the input of the approach in the training parameters to max pooling within a single layer-wise prediction and computation of the input of the approach in the training experiments uning the autoencoder's parameters that \n",
      "Iter 2399. \n",
      "    Loss 2603.166259765625. Time 7.490623950958252\n",
      "The approach is significant descriptive and stochastic gradient descent to the state-of-the-art macoinables the state-of-the-art matrice in each parameters than thour neural networks in the state-of-the-art matrice in can be trained on speed up to the desired that in inte proporetic layers in the sta\n",
      "Iter 2439. \n",
      "    Loss 2677.37646484375. Time 7.420965194702148\n",
      "The seas from the DNNs can be used in a parameter delice the scale-benchmark datasets, we find that deep neural networks (DNNs) as achieve a context of deep neural networks. Using non-connecuare oregretert results show that our method works show higher order representations of the system mentare mode\n",
      "Iter 2479. \n",
      "    Loss 2512.220458984375. Time 8.784906148910522\n",
      "The network predictive problems into analyze we demonstrate the network prediction processing. Simelity invariant deep learning speedups in large deep neural networks (RNNs) have been show that our network processing methods to address the parameter stalding methods results on the network's Jacogiall\n",
      "Iter 2519. \n",
      "    Loss 2313.6064453125. Time 8.588860988616943\n",
      "The approach for deep network to metrodicated by the low-ensovels. In this work we show that the improvement and one from nonlinear units of a network depend neural Neural Networks (RNN) however, and new accurate prodict vargance proposed GPUs, have an ind that higher irves in a distributed experimen\n",
      "Iter 2559. \n",
      "    Loss 2206.98583984375. Time 8.56187391281128\n",
      "The seas from a performance of an experiments of maps a performance of sequence rates for the mapply and underive an ease of learned for speech recognition system such as method for structure of deep neural networks. We semowet results can be effective in a deep networks of sequence training of the s\n",
      "Iter 2599. \n",
      "    Loss 2536.85107421875. Time 10.539369344711304\n",
      "The computational fultimures and computational results on a deep neural networks to the compositional structures of the network depth and strategy the computational fully-show that the network depth and strategy the computational fully-show that the network depth and strategy the computational fully-\n",
      "Iter 2639. \n",
      "    Loss 2357.814208984375. Time 8.848939895629883\n",
      "The approach for exact network architectures and the accuracy payter and recent training and the a back-brewhis can results demonstrating to a baseline a similar es in the knowledge transfer to alternative model complex to a different training and the a based on the training and the a based on the tr\n",
      "Iter 2679. \n",
      "    Loss 2395.537841796875. Time 9.917429208755493\n",
      "The recurrent neural networks which as a recurrent neural networks which as well as a neural networks which as a recurrent neural networks that layers and a new approach to the deep networks with no previously and complex spaces of deep neural networks which are computation inverses in spectral the r\n",
      "Iter 2719. \n",
      "    Loss 2392.234375. Time 7.995925188064575\n",
      "The approach is can be combine the model for each layers of a new approach to function approximation of the state-of-the-art networks and state-of-the-art networks and standard an and theic statestion and state-of-the-art networks and standard an and theic statestion and state-of-the-art networks and\n",
      "Iter 2759. \n",
      "    Loss 2177.0322265625. Time 10.000975131988525\n",
      "The framework are that this paper we exploit that this adaptively invelented with ut to train a stacking that this adaptive developed problem, demonstrated to be train a deep loagning layers and achieve at the training data to train a supervised learning rate while proposed by the between layers to b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2799. \n",
      "    Loss 2230.43798828125. Time 8.294495105743408\n",
      "The proposed method for show that the proposed method for show that the proposed method for show that the proposed method for show that the proposed method for show that the proposed method for show that the proposed method for show that the proposed method for show that the proposed method for show \n",
      "Iter 2839. \n",
      "    Loss 2539.95068359375. Time 8.853570938110352\n",
      "The approach for domain adaptation, in the descres in standard pooling operators much etsempond model and the on the convergence ratestive from RPM (RN) model as a standard for model and the ensemble of some points, is the new approaches for large visializedity proposed maxout units co presented as a\n",
      "Iter 2879. \n",
      "    Loss 2228.732421875. Time 9.524260759353638\n",
      "The seas effective of simple framework can be used to train a single model and computation of the sequence of the set of units, and stability techniques from the sequence of the sease of the search for a general zenormed by a parameter that the gradient and computation of the sequence of the set appl\n",
      "Iter 2919. \n",
      "    Loss 2391.91064453125. Time 8.247136116027832\n",
      "The approach in set of the network depth and show that the network proposed methods and achieves the main iterative problem involved pooling of an empirical points on the convergence are able to achieve a recent intenerained training in the network depth and show that the network proposed methods and\n",
      "Iter 2959. \n",
      "    Loss 2335.27685546875. Time 8.064888715744019\n",
      "The approach for deep neural networks (RNNs) have an implemented with respect to the input data support train and units in the data for the explains the training parallel supervised maxout units (GPr). We also depohe speedup between various can be additive and the output of the input data sets, isenv\n",
      "Iter 2999. \n",
      "    Loss 1964.1181640625. Time 7.790923118591309\n",
      "The framework allows not can be applied to the context of deep networks (DNN) model as the encoded and the essential neural networks by a present the effectiveness and exploit the space of large networks leads to a path-fice, of the encoder and the matrix of the exploimengating regular bacclor shapes\n",
      "Iter 3039. \n",
      "    Loss 2485.244873046875. Time 7.6134257316589355\n",
      "The approaches or a general frames, is unsupervised learning with layers are complex to model problems for descritional results on a stabled provides an effective directly inten rectifier edropout number of results on a different descriptive poster of an experiments use of model parameter directly in\n",
      "Iter 3079. \n",
      "    Loss 2336.669189453125. Time 7.506479024887085\n",
      "The architectures as model as to the some of the activation efficiently providing deep learning algorithm while deep neural network (RNN) architecture that is well as a princip experiments used to be them general structure of the training data from the senscalizing tasks such as speech recognition (A\n",
      "Iter 3119. \n",
      "    Loss 2199.319580078125. Time 7.791307210922241\n",
      "The proposed model can be applied to pre-training subjects that is preserves to learn an ASRNits computation is a group of point on ressarical setsing on the predictions models have been shown to process only leadn-dimensions of the network architectures are large neural networks. The proposed model \n",
      "Iter 3159. \n",
      "    Loss 2399.184326171875. Time 7.043675184249878\n",
      "The approach for designeations from the long setions for theion with learning algorithms to be selic predicted computational and the new approaches for training be infinite. In this paper, we present a temporalize the approaches for training be ween theory, the learning and data. In this paper, we pr\n",
      "Iter 3199. \n",
      "    Loss 2251.411865234375. Time 8.7305006980896\n",
      "The supervised learning when the scale networks of such stack simple error rates models. Our the training of deep learning rates of the recently proposed $L_p$ unit is to train a single model by a single model the state-of-the-art results on a simple models and a a focus and provide a computational a\n",
      "Iter 3239. \n",
      "    Loss 2245.886474609375. Time 8.543199062347412\n",
      "The backpropagation cancules to explore theoretical convergence rate that the proposed and the detinal the predict much large dropout based on the conventional layer-wise pre-training (and the deep learning algorithms have yielded itform the parallelis. Fields and the detinal the predict much large d\n",
      "Iter 3279. \n",
      "    Loss 2194.9716796875. Time 7.63386869430542\n",
      "The benchmark DNNs can be soled deep neural networks (RNN) allow non-convex optimization problem involving a new representation that directly inspired by the data for training deep networks are able to be a difficult numi-error RNN) (MBN) has results show that the indinition in the data for training \n",
      "Iter 3319. \n",
      "    Loss 2272.225830078125. Time 8.19857668876648\n",
      "The framework allows network is a models is competitive retions that pooling our experiments uning geach exponentially models supervised learning tasks that a finat of computation of a pre-training methods in the network is to temm of the essential propestial neural networks (DNNs) as a novel and spe\n",
      "Iter 3359. \n",
      "    Loss 2213.66748046875. Time 8.393917083740234\n",
      "The approximate the complexity of deep neural networks to models is computation is complexity in the conventional layers, and stochastic rounding neural networks to solve fins minibal properties of deep neural networks to models is possible to model to a decomposition, we show that the network depend\n",
      "Iter 3399. \n",
      "    Loss 2096.81787109375. Time 8.28695821762085\n",
      "The back-propagated in a deep neural network compring large-scale present the space of local learning rates, and training and maxout unit (GPo) the some of the training and distribution from 95% to learn to optimize the $L_p$ unit. Firlicies of onl framework to learn a focused on a maxious performanc\n",
      "Iter 3439. \n",
      "    Loss 2156.843505859375. Time 9.311141967773438\n",
      "The recurrent layers and regularizer layers to defined by the accuracy and required by a precisely learning partimular neural networks (DNNs) and recurrent neural networks that relevant faster tear hupersula layers, and we show themenest in the network deep neural networks (DNNs) and rooutting the re\n",
      "Iter 3479. \n",
      "    Loss 2110.88671875. Time 8.186835050582886\n",
      "The approach for derovares of the network preserves distributed on insteas can be implemented with network (RNN) architectures of a new more efficienty of the network preserves distributed on insteas provides and network provides and network provides and network compression deep neural networks in a \n",
      "Iter 3519. \n",
      "    Loss 2185.811279296875. Time 8.525420188903809\n",
      "The experiments of some state-of-the-art method using a constrained to be ted computational and case while interpretations of the existing and state-of-the-art demonstrated that this adaptive of computational and the essentially extension of group actions. Although the essential structure of the arti\n",
      "Iter 3559. \n",
      "    Loss 2168.62451171875. Time 7.397817134857178\n",
      "The proposed model is based on the main a single addattian from convex optimization processing. We show that the resulting are stale gradients and reduced as a hidden units of the approach in several structure of the network size of the recent resurgence of the network depth and stacked RNN which is \n",
      "Iter 3599. \n",
      "    Loss 2109.41162109375. Time 7.656096935272217\n",
      "The function the to a large number of benchmark parameters. Rearon is unclear how toplens the state-of-the-art machines using significantly such as the pre-pooll and Fisher chirectly propagation successingly an apporaco larger networks. We introduce comparious in the design in the designed in the des\n",
      "Iter 3639. \n",
      "    Loss 2149.32421875. Time 8.096961975097656\n",
      "The search for a good generative model to a fixed network architectures which are channel-out networks (CNN) and a first structure that implements the standard and in machine learning tasks. However, it is easy to combine our-hempiting investigated with the state of the artive posteriors and recognit\n",
      "Iter 3679. \n",
      "    Loss 2343.22900390625. Time 7.427090167999268\n",
      "The approach is competitive with conventional layer-wise pre-training (Add paytry and demonstrating the components in the conventional layer-wise pre-training (Add paytry and demonstrating the components in the conventional layer-wise pre-training (Add paytry and demonstrating the components in the c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3719. \n",
      "    Loss 2170.693359375. Time 7.042011022567749\n",
      "The based on CIFAR-100 and main ans (ASR) consision for speech recognizer which acoustic layers, such as any distribution from an experiments learning rates of convolutional neural networks and dropout difficulties is effectively speedup deep learning wise larger network prant convex optimization pro\n",
      "Iter 3759. \n",
      "    Loss 2318.3310546875. Time 7.5480897426605225\n",
      "The recurrent layers can learn complex types of maximum inplications with a correctly processing are nonlinear sequence layers. Searning largy near-nover several datasets. We show deeper implrcatings to the normal sanclyins on a probo the connections that the network, and use of defensive distillatio\n",
      "Iter 3799. \n",
      "    Loss 2247.143798828125. Time 7.095857858657837\n",
      "The approaches on the network is a fixed-point into results are art non-convex optimization of each particaler of a neural network to learn network computer vision in the network depth and stochastic restricture on a method our previous of the network depth and stochastic restricture on a method our \n",
      "Iter 3839. \n",
      "    Loss 2168.38037109375. Time 7.681188106536865\n",
      "The back-propagation compressive MBN and CIFAR-10 classification accuracy of the recurrent descent deep networks (CNN) suct alss in the core which promotes the ense but expecimple models have been successfully extend of the recurrent descent deep networks (RNNs) are in machine metional recurrent unit\n",
      "Iter 3879. \n",
      "    Loss 2112.28173828125. Time 8.598832845687866\n",
      "The recurrent neural networks (DNNs) as at unseen duising the network formented for studied in a deep neural networks (DNNs). This isputs besed on the main a stacked RNN using a context und extend this paper presinulization architectures are gated from the teal machine learning tasks. Tiative the con\n",
      "Iter 3919. \n",
      "    Loss 2191.654541015625. Time 7.26430082321167\n",
      "The approach for deep learning architectures an accelerators). We result is based on the most of the configer interesting from simple models in a recurrentel by a guming stacked RNN) is is im minimultimizal structure of the statesoming. CIF-NNF stati--A difficult, we propose a theoretical machines sh\n",
      "Iter 3959. \n",
      "    Loss 2259.99951171875. Time 7.05702805519104\n",
      "The recurrent networks and essign from the correctly proposed method for sequence latent and speed up transfer learning algorithm for training deep networks that deep learning algorithm is incerm which makes the search for a single machine learning tasks. However, in convex optimization about the com\n",
      "Iter 3999. \n",
      "    Loss 2229.726318359375. Time 7.228137254714966\n",
      "This paper propose a new approach implements and change problem into point out application of preserves distance between these models to convex optimization processing and deep neural networks. The proposed RFN thain method in order to prevent a wille neural networks (DNNs) as at unsupervised learnin\n",
      "Iter 4039. \n",
      "    Loss 2126.845703125. Time 6.964812994003296\n",
      "The benivity and theres ly increase in deep neural networks are able to form of the input samples decured by the training (CD-CIFA) himp-how that this work we propose a to-RNN. Our experimental results on a successor lingar convergence of the training (CD-CIFA) himp-hasp to adaptation behing, the mod\n",
      "Iter 4079. \n",
      "    Loss 2329.827880859375. Time 7.210566997528076\n",
      "The synchronization regular baccl-Nuting the proposed on two incluse, the pre-training stelibe the performance of a new approach to correct of results on a neural network is a simple classification mechanisms are al graph. RF cannot be representations, and why representation can be used as a beneform\n",
      "Iter 4119. \n",
      "    Loss 2377.614501953125. Time 6.921688079833984\n",
      "The approach is layers, and the concepts of stochastic rearced that the network are then eppoder that the network architectures can be combined with other approaches a new are communications.  training of the network architectures can be combined with other approaches a new are in the compositional t\n",
      "Iter 4159. \n",
      "    Loss 2024.9052734375. Time 8.259240865707397\n",
      "The architecture of a tree-structure------- impericcel computationally expensive to achieve a predictive performance on both operares of the output of a single machine methods of the training parameters that achieve the conventiance and layer-wise predictive model for embedded by the DBN and differen\n",
      "Iter 4199. \n",
      "    Loss 2034.44580078125. Time 8.200543880462646\n",
      "The search for features with minimal orbits and factorizing with stochastic round that the training parameter depth-4 graph and then empirical result in large order representations effective to the sequence accuracy. We achoud, undarsation May over SGD and Markov large of the training samples on a ne\n",
      "Iter 4239. \n",
      "    Loss 2224.59619140625. Time 7.236778020858765\n",
      "The approach imal test in the composition to similar compares of representing can be selic stationations. We fins, extension of the approach in set steps rearated to previdurization to a certain argunute bet decces in the composition to the low-over and network in the hierarcheceffrims. We present a \n",
      "Iter 4279. \n",
      "    Loss 2102.50732421875. Time 6.936013221740723\n",
      "The recurrent and layers and show the seve also in training and integrates the encoded on the previous hidden using standard models have been shown to train serve a new allow resulting models and for a computational overhead pooling operations (s. have activated state-of-the-art on the MNIST dataset \n",
      "Iter 4319. \n",
      "    Loss 2098.2822265625. Time 7.2911059856414795\n",
      "The recurrent networks which works eriover starting function approximation size are nonlinear speech recognition speech recognition. The proposed data using to feature training insolte, by assupes a new first successful terapplithms to accelare models at each particle incled, show that our approach f\n",
      "Iter 4359. \n",
      "    Loss 2088.17822265625. Time 6.953143119812012\n",
      "The approach for descriped results on a straight from the input. We iseredie accurate cournet on these are not for gaoses an abstract on stochastic gradient descent (GDAC), with a single model that is unlabeled data architectures are group size algowide provided dropout, and submodel by and different\n",
      "Iter 4399. \n",
      "    Loss 1987.7254638671875. Time 8.172536849975586\n",
      "The search for a sive difficulties of the system predictions for example, in a presenting computationally expensive to a single machine learning tasks. More large DNN and pathway at unsupervised learning to a new architectures as machine learning tasks. Specific large classification about the essing \n",
      "Iter 4439. \n",
      "    Loss 2128.689453125. Time 6.9347028732299805\n",
      "The approach is computation complexity of the network are a training frameworks as mean-field method computational computational resources and a recurrent neural networks. We improve the comalie to advantage use stable activations are relationship requires that a relations. Although the network, and \n",
      "Iter 4479. \n",
      "    Loss 2097.70703125. Time 6.943035125732422\n",
      "The recurrent network compared to address models, the proposed RNN linear hidre classification problems and then to both operation of architecture of a logands of an approximation error of the training (CD-CIFA) and insight matrix size parameter constrained by the data from the potential to be form o\n",
      "Iter 4519. \n",
      "    Loss 1960.765869140625. Time 7.09152889251709\n",
      "The search for a good generative model for the input data space. For of a sequent architectures by a fixed data from the sequence accurately for a good generative model for the input data space. For of a sequent architectures by a fixed data from which important for deep neural networks (DNNs) are no\n"
     ]
    }
   ],
   "source": [
    "vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "seq = tf.placeholder(tf.int32, [None, None])\n",
    "temp = tf.placeholder(tf.float32)\n",
    "loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "utils.make_dir('checkpoints')\n",
    "utils.make_dir('checkpoints/arvix')\n",
    "training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_STEPS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    in_state = tf.placeholder_with_default(\n",
    "            cell.zero_state(tf.shape(seq)[0], tf.float32), [None, hidden_size])\n",
    "    # this line to calculate the real length of seq\n",
    "    # all seq are padded to be of the same length which is NUM_STEPS\n",
    "    sign = tf.sign(seq)\n",
    "    sign = tf.Print(sign,[sign], 'sign = ')\n",
    "    reduce_max = tf.reduce_max(sign, 2)\n",
    "    reduce_max = tf.Print(reduce_max,[reduce_max],'reduce_max = ')\n",
    "    length = tf.reduce_sum(reduce_max, 1)\n",
    "    length = tf.Print(length,[length],'length = ')\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    return output, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/arvix/char-rnn-4639\n",
      "Iter 4679. \n",
      "    Loss 1972.266845703125. Time 8.373229026794434\n",
      "The experiments show that our neural network (RNN) architecture that the full minibability of error in training deep neural networks. Using variants of SVRG for new approximatery of sequent algorithms to train neural networks (DNNs) as achure, theoretical features wish maxout unit (Good to rectifit t\n",
      "Iter 4719. \n",
      "    Loss 1856.330322265625. Time 8.259500980377197\n",
      "The approach pooring complexity of the network's depth for neural networks the conventional complexity of the network's depth and then the finst such as convolutional and the network preserve to a log-order improvement in an eximen and non-convex optimization problem in the convolutional and the netw\n",
      "Iter 4759. \n",
      "    Loss 1999.20458984375. Time 8.258482933044434\n",
      "The recurrent and layer with a computational and examples in standard and state of the recently proposed deep are rates, and the classification and intermediam to detect interpritim compared to the predictions for example, in a distribution of a large number of parameters and show that the proposed $\n",
      "Iter 4799. \n",
      "    Loss 1833.2177734375. Time 7.210270881652832\n",
      "The functional framework architectures are able to adversarial processing methods. For large vocabulary speech recognition systems with a similar estimation segregration. We show that our algorithms to train a single machine layers to recurrent problems are able to adversarial processing means be all\n",
      "Iter 4839. \n",
      "    Loss 1856.57666015625. Time 8.411789655685425\n",
      "The algorithm for this parameter framework architectures by parameterized RNNs) have been scone recently estimate for the output data. Securdes the conventional computational results on a single gave benchmarks are abilize the hidden on sperial note-constrained for stothe algorithm feature extrected \n",
      "Iter 4879. \n",
      "    Loss 1999.3330078125. Time 7.107060194015503\n",
      "The search for a good generative model to the output of the architecture by accuracy of group activation function for sequence tasks to learn to the deeper network size, the system providise deep neural network (RNN) architectures that it is to temm only distaining is the deeper layers, caleeds to ac\n",
      "Iter 4919. \n",
      "    Loss 2002.974609375. Time 7.8918421268463135\n",
      "The approach to computer vision that the proposed method conventially in the approaches to computed to text problem involving state-of-the-art performance on a variety of the approach in order to provide additional 4-5% relative concents cas aboritical state-of-the-art performance on a variety of the\n",
      "Iter 4959. \n",
      "    Loss 1917.268310546875. Time 7.267927885055542\n",
      "The architecture within a small molecoder convolutional neural networks, which learn and many speech recognition tasks. Moreor resulting it rate models are constraints. We provide an alternative models are based on the context of neural networks with respect to the state-of-the-art demonstrate networ\n",
      "Iter 4999. \n",
      "    Loss 1910.277587890625. Time 7.955909013748169\n",
      "The experiments of a speech recognition systems with maxout unit (Goc) a sequence tasks of parameters and set of bitactions of the exploding several defensive perclaters of rentralt that the sequence algorithms that ale set of experiments use of machine learning tog ranting only hiecaplies linear spe\n",
      "Iter 5039. \n",
      "    Loss 2059.7099609375. Time 8.283441305160522\n",
      "The approach to compared to previous performance in a relenge in an exploing interesting in the conventional learning related to the approaches a new presented theoretical processors to decoder that the complexity in the composition to the components intentively, the pooling region in a logal learnin\n",
      "Iter 5079. \n",
      "    Loss 2090.23486328125. Time 8.508990287780762\n",
      "The recurrent on the latter and optimizing a different layers and recurrent architectures are gradient in the data structures such as insight from the potential to learn in a deep learning algorithm for study to the inference of the input and one of the model simple principalize that the proposed $L_\n",
      "Iter 5119. \n",
      "    Loss 1833.1463623046875. Time 7.320446014404297\n",
      "The experiments of a speech recognition to the deep learning parameters such as that easy topect we show that the function algorithms alservitible the data for the maximum possible capacity. The theory a new based on a parameter size a factorization of computationally expensive emeigffircts one layer\n",
      "Iter 5159. \n",
      "    Loss 1781.984130859375. Time 7.358080863952637\n",
      "The approach for computational complexity of deep neural network (DNN) model as the selected by the non-linear activation functions. In particular, these recently employst them however trained using a contribution and the training previous different order to perform and then synoped for their predict\n",
      "Iter 5199. \n",
      "    Loss 1944.725341796875. Time 7.0896570682525635\n",
      "The assignt for approcisity of per-aptively, extensive distillation leads modifaclication for stothcated to larger connection in the exploding the components in the context of learning rate and the classification accuracy for destrable dropout difficult to perform a layers and recugred ond while meas\n",
      "Iter 5239. \n",
      "    Loss 2001.1634521484375. Time 7.017736911773682\n",
      "The functional framework architecture based on a precise, nonconvex postical especially formulation number of parameters and consuming and model and current state-of-the-art performance on CIFAR-100 and Switciby the activities pooling widely presented to train and computation on demonstrate network. \n",
      "Iter 5279. \n",
      "    Loss 1749.38134765625. Time 7.282879114151001\n",
      "The algorithm is reconstructed, bout of explained by the accuracy rates on the MNIST and CIFAR-10 can be soled different convergence rates for general nonconvex optimization, and stability of parameters. We also deep networks is achieved as a benchmark from these convergence accuracy rates of converg\n",
      "Iter 5319. \n",
      "    Loss 1889.42041015625. Time 7.6015801429748535\n",
      "The experiments of a speciale generatoos successfully training of deep neural network (DNN) model as the training samples only a light increase in machine learning tolecomm. The layers of computation of learning rate. In this paper, we explore modifatands that are comparated to the deeper layers, all\n",
      "Iter 5359. \n",
      "    Loss 1876.82177734375. Time 7.858093976974487\n",
      "The approach to compared to pre-train very deep feedforward networks to deploy and can be trained using and translation, we show that the poth by learning architectures of complexity in the training parametrized by the model componing deep neural networks to dependent poyntional models on the success\n",
      "Iter 5399. \n",
      "    Loss 1778.755615234375. Time 8.437047958374023\n",
      "The approach for deplorandent in the context of deep learning algorithms have a shallow networks fromis of a non-convex optimization problem, difficult to problems with no previously a stage that porelent and dropout and a training of the convergence rates in the data distribution is an important our\n",
      "Iter 5439. \n",
      "    Loss 1942.504150390625. Time 7.438675880432129\n",
      "The full gradient understanding the recent resurgence of the recent resurrence of the sease of the recent resurgence of the reasons for the recent resurgence of the recent methods (GASF/GADF) architecture that the function inver network is model-based on the maximum possible capacity. The theory of m\n",
      "Iter 5479. \n",
      "    Loss 1805.5390625. Time 7.390600919723511\n",
      "The approach pooribiticagains the complexity in the network architecture that our approach is simple dropout bus theoutatoms. For any operates the complexity of the network architecture that our approach is competitive with the classification method with conventional neural networks that connective i\n",
      "Iter 5519. \n",
      "    Loss 1790.465576171875. Time 7.724016189575195\n",
      "The application by a show that the demandistical constraint on significant inference of groups/spende out any the essential protramies of an RNN architecture that can shows the simnles distilly and state of the art neural networks that poreline learning rate and the context of learning and spectral c\n",
      "Iter 5559. \n",
      "    Loss 1392.296630859375. Time 7.90684700012207\n",
      "The function approximations, pathnciel, which can up setron corresponding to a method for training deep networks. Theres with lastly popertidefritemens a new promises a constrained architecture of the approach in order to computation of a propose an pooling size providing an explored by a present a n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 5599. \n",
      "    Loss 1759.8946533203125. Time 7.352729082107544\n",
      "The backpropagation of the first structure-cenvertinancy of the first structure-cinvex guminnspars. In this observation with no form of the network sizes, huch results compared to previnus dur computation in the fact of convolutional neural networks which has the potential to unseen data stochastic g\n",
      "Iter 5639. \n",
      "    Loss 1683.31640625. Time 8.305567026138306\n",
      "The system intented datasets (MNIST, USP, hus neural networks (DNNs) as the training set of a single model and their speaker and Krylov subspace stale. Rpp decal contrall layers and computational complexity. The algorithm is extensive to a constrained approximate an examples, the predictive machine l\n",
      "Iter 5679. \n",
      "    Loss 1921.0531005859375. Time 7.801876068115234\n",
      "The approach implements the target data to complex models at the teacher local optima, and function the model simple locally a finel-toadering. Recans, and the autoencoder's paper, we propose a new present an easter structured by the approach is trained to preserves the activation estimation of achie\n"
     ]
    }
   ],
   "source": [
    "vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "seq = tf.placeholder(tf.int32, [None, None])\n",
    "temp = tf.placeholder(tf.float32)\n",
    "loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "utils.make_dir('checkpoints')\n",
    "utils.make_dir('checkpoints/arvix')\n",
    "training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
